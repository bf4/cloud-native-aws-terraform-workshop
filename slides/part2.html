<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>reveal.js</title>

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/8thlight.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h1>AWS / Terraform Workshop Series #2</h1>
          <h2>Distributed Systems / Autoscaling / Load Balancers</h2>
          <ul>
            <li>Colin Jones, CTO at 8th Light</li>
            <li><a href="https://twitter.com/trptcolin">@trptcolin</a></li>
            <li>based on <a href="https://github.com/18F">18F</a>'s public-domain workshop</li>
          </ul>
        </section>
        <section>
          <h2>Today's Goals</h2>
          <ul>
            <li>Understand architectural implications of building distributed systems</li>
            <li>Know how the cloud enables building high availability systems</li>
            <li>Understand the trade-offs involved in building cloud native systems</li>
            <li>Understand the architecture of auto scaling groups</li>
            <li>Know how to create an auto scaling group with Terraform</li>
            <li>Be able to configure hosts created by auto scaling groups</li>
          </ul>
        </section>
        <section>
          <blockquote>Operations at web scale is the ability to consistently create and deploy reliable software to an unreliable platform that scales horizontally.</blockquote>
          &mdash; Jesse Robbins, "Master of Disaster" at Amazon, <a href="http://oreil.ly/1HRKUVE">Operations is a Competitive Advantage</a> (O'Reilly)
        </section>
        <section>
          <h2>Unreliable platform</h2>
          <img style="width: 100%" src="images/oh_noes.png" />
          <aside class="notes">
            This will happen in your physical datacenter too! Amazon mitigates it somewhat with EBS volumes, which have an annual failure rate of 0.1%-0.2%.
          </aside>
        </section>
        <section>
          <h2>Unreliable platform</h2>
          <ul>
            <li class="fragment">If your system is on the internet, you're on an unreliable platform.</li>
            <li class="fragment">Amazon regions have a 99.95% SLA for availability (including RDS)...</li>
            <li class="fragment">...but only if you use multiple availability zones.</li>
          </ul>
          <aside class="notes">
             Networks are subject to partitions. Eric Brewer's CAP theorem tells us that we must trade off consistency vs availability - if we want > 99.95% availability, we can't depend on AWS's multi-AZ primitives. This is not academic - a whole AWS region became unavailable. The Netflix blog post on the 2011 AWS outage is worth reading: http://techblog.netflix.com/2011/04/lessons-netflix-learned-from-aws-outage.html
          </aside>
        </section>
        <section>
          <h2>Scaling horizontally</h2>
          <img style="width: 95%" src="images/nosql.png"/>
          <aside class="notes">
            Clouds are elastic - you can scale on demand. This must be architected for. Only a "shared nothing" architecture truly allows this. But, at least, we need to keep session state out of the application tier. Image credit: http://howfuckedismydatabase.com/nosql/
          </aside>
        </section>
        <section>
          <h2>Destroy works of art</h2>
          <ul>
            <li class="fragment">Hosts should be <em>disposable</em>.</li>
            <li class="fragment">Provisioning new hosts should be fully automated using versioned config.</li>
            <li class="fragment">Our infrastructure should automatically detect failures and kill.</li>
            <li class="fragment">If there's insufficient volatility, we should introduce more (e.g. chaos monkey).</li>
          </ul>
        </section>
        <section>
          <h2>Introduce failure</h2>
          <img style="width:100%" src="images/chubby.jpg">
          <i>Site Reliability Engineering: How Google Runs Production Systems</i>, p39.
          </ul>
        </section>
        <section>
          <h2>Twelve Factor Apps</h2>
          <ul>
            <li>Declarative formats for setup automation, to minimize time and cost for new developers;</li>
            <li>Clean contract with the underlying operating system, offering maximum portability;</li>
            <li>Suitable for deployment on modern cloud platforms</li>
            <li>Minimize divergence between development and production, enabling continuous deployment;</li>
            <li>Can scale up without significant changes to tooling, architecture, or development practices.</li>
          </ul>
          &mdash; Adam Wiggins, <a href="https://12factor.net">https://12factor.net/</a>
          <aside class="notes">
            Basically, a combination of cloud-native architecture plus continuous delivery.
          </aside>
        </section>
        <section>
          <h2>Cloud Native apps</h2>
          <ul>
            <li class="fragment">Make apps stateless so they can scale horizontally and fail arbitrarily;</li>
            <li class="fragment">Make apps disposable so we can start them up fast and automatically;</li>
            <li class="fragment">Separate packages from configuration information for all resources (remote or attached);</li>
            <li class="fragment">Each <a href="http://martinfowler.com/bliki/BoundedContext.html">bounded context</a> gets its own app;</li>
            <li class="fragment">Apps are independently testable and deployable;</li>
            <li class="fragment">Treat logging as a remote service;</li>
            <li class="fragment">Instrument everything! Localizing problems is hard in distributed systems, and some issues are emergent.</li>
          </ul>
        </section>
        <section>
          <img src="images/ha_aws_system.png" alt="system diagram" height="60%" width="60%" border="0"/>
          <aside class="notes">
            This is the networking infrastructure we will have built by the end of the class and the key components. Note this doesn't include monitoring, the S3 bucket we deploy from, or the security groups.
          </aside>
        </section>
        <section>
          <h2>Auto scaling groups</h2>
          <img src="http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/images/tutorial_as_elb_architecture.png" style="float:right" width="45%"/>
          <ul style="width:45%">
            <li class="fragment">At least two availability zones</li>
            <li class="fragment">Load balancer</li>
            <li class="fragment">Auto scaling group</li>
            <li class="fragment">Launch configuration</li>
          </ul>
        </section>
        <section>
          <h2>Elastic Load balancers in AWS</h2>
          <ul>
            <li class="fragment">Two types: classic and ALBs</li>
            <li class="fragment">ALB (application load balancer): route based on OSI layer 7</li>
            <li class="fragment">For example: headers, content, path</li>
            <li class="fragment">Also supports HTTP/2 and Websockets</li>
            <li class="fragment">ALBs also have request tracing (create a unique header per request)</li>
          </ul>
        </section>
        <section>
          <h2>Second availability zone</h2>
          <pre><code data-noescape>resource "aws_subnet" "public_subnet_2" {
    vpc_id = "${aws_vpc.workshop_vpc.id}"
    availability_zone = "us-east-1d"
    cidr_block = "<mark>10.0.X.64/26</mark>"
}

resource "aws_route_table_association" "route_subnet_2" {
    subnet_id = "${aws_subnet.public_subnet_2.id}"
    route_table_id = "${aws_route_table.workshop_route_table.id}"
}</code></pre>
          <aside class="notes">
            Don't forget to add a route for it...
          </aside>
        </section>
        <section>
          <h2>Security groups: ALB</h2>
          <pre><code data-noescape>resource "aws_security_group" "alb_sg" {
    vpc_id = "${aws_vpc.workshop_vpc.id}"

    ingress {
        from_port = 80
        to_port = 80
        protocol = "tcp"
        cidr_blocks = ["0.0.0.0/0"]
    }

    egress {
        from_port = 80
        to_port = 80
        protocol = "tcp"
        cidr_blocks = ["<mark>10.0.X.0/24</mark>"]
    }
}</code></pre>
        </section>
        <section>
          <h2>Security groups: Web hosts</h2>
          <pre><code data-noescape>resource "aws_security_group" "web_sg" {
    vpc_id = "${aws_vpc.workshop_vpc.id}"

    ingress {
        from_port = 80
        to_port = 80
        protocol = "tcp"
        security_groups = [ "${aws_security_group.alb_sg.id}" ]
    }

    egress {
        from_port = 0
        to_port = 0
        protocol = "-1"
        cidr_blocks = ["0.0.0.0/0"]
    }
}</code></pre>
        </section>
        <section>
          <h2>Security groups: Web hosts (pt 2)</h2>
          <pre><code data-noescape>    ingress {
        from_port = 22
        to_port = 22
        protocol = "tcp"
        security_groups = [ "${aws_security_group.bastion_sg.id}" ]
    }</code></pre>
        </section>
        <section>
          <h2>Application load balancer</h2>
          <pre><code data-noescape>resource "aws_alb" "workshop_alb" {
  name = "workshop-alb-<mark>[name]</mark>"
  subnets = ["${aws_subnet.public_subnet_1.id}",
             "${aws_subnet.public_subnet_2.id}"]
  security_groups = ["${aws_security_group.alb_sg.id}"]
}

resource "aws_alb_target_group" "workshop_alb" {
  name = "workshop-alb-target-<mark>[name]</mark>"
  vpc_id = "${aws_vpc.workshop_vpc.id}"
  port = 80
  protocol = "HTTP"
  health_check {
    matcher = "200,301"
  }
}
</code></pre>
          <aside class="notes">
            Note - the target group is not going to show healthy at first. The default install page on Apache httpd sends you a 403. 403 is not a valid "heatlhy" code for a target group.
          </aside>
        </section>
        <section>
          <h2>ALB target group</h2>
<pre><code data-noescape>resource "aws_alb_listener" "workshop_alb_http" {
  load_balancer_arn = "${aws_alb.workshop_alb.arn}"
  port = 80
  protocol = "HTTP"
  default_action {
    target_group_arn = "${aws_alb_target_group.workshop_alb.arn}"
    type = "forward"
  }
}

output "alb.dns" {
    value = "${aws_alb.workshop_alb.dns_name}"
}</code></pre>
  <aside class="notes">
    The output variable <code>dns_name</code> gives us the FQDN of the load balancer. This is a major advantage of using ELBs - no IP addresses in your DNS entries.
  </aside>
        </section>
        <section>
          <h2>Launch configuration</h2>
          <pre><code data-noescape>resource "aws_launch_configuration" "workshop_launch_conf" {
    image_id = "ami-c481fad3"
    instance_type = "t2.micro"
    key_name = "<mark>[key name]</mark>"
    security_groups = ["${aws_security_group.web_sg.id}"]
    associate_public_ip_address = true
    lifecycle {
        create_before_destroy = true
    }
    user_data = &lt;&lt;EOF
#!/usr/bin/env bash
yum update -y
yum install -y httpd24
service httpd start
EOF
}</code></pre>
        </section>
        <section>
          <h2>Auto Scaling Group</h2>
          <pre><code data-noescape>resource "aws_autoscaling_group" "workshop_autoscale" {
    vpc_zone_identifier = ["${aws_subnet.public_subnet_1.id}",
                           "${aws_subnet.public_subnet_2.id}"]
    min_size = 2
    max_size = 2
    health_check_type = "EC2"
    health_check_grace_period = 300
    launch_configuration = "${aws_launch_configuration.workshop_launch_conf.id}"
    target_group_arns = ["${aws_alb_target_group.workshop_alb.arn}"]
    enabled_metrics = ["GroupInServiceInstances"]
}</code></pre>
        </section>
        <section>
          <h2>Review</h2>
          <ul>
            <li class="fragment">Internet systems are distributed systems</li>
            <li class="fragment">We need to architect for availability in the face of an unreliable platform</li>
            <li class="fragment">Design for stateless, disposable apps and hosts that can be independently tested and deployed</li>
            <li class="fragment">Be able to provision in a fully automated fashion from version control</li>
            <li class="fragment">We stood up an AWS load balancer and Auto Scaling Group</li>
            <li class="fragment">We deployed all the infrastructure required for a highly available web app</li>
            <li class="fragment">We provisioned everything purely from versioned Terraform config</li>
          </ul>
        </section>
      </div>
    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>
      // More info https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        history: true,

        width: "95%",
        height: "100%",
        margin: 0,
        minScale: 1,
        maxScale: 1,

        // More info https://github.com/hakimel/reveal.js#dependencies
        dependencies: [
          { src: 'plugin/markdown/marked.js' },
          { src: 'plugin/markdown/markdown.js' },
          { src: 'plugin/notes/notes.js', async: true },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
        ]
      });
    </script>
  </body>
</html>
