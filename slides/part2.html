<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>reveal.js</title>

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/8thlight.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h1>AWS / Terraform Workshop Series #2</h1>
          <h2>Distributed Systems / Autoscaling / Load Balancers</h2>
          <ul>
            <li>Colin Jones, CTO at 8th Light</li>
            <li><a href="https://twitter.com/trptcolin">@trptcolin</a></li>
            <li>based on <a href="https://github.com/18F">18F</a>'s public-domain workshop</li>
          </ul>
        </section>
        <section>
          <h2>Today's Goals</h2>
          <ul>
            <li>Understand architectural implications of building distributed systems</li>
            <li>Know how the cloud enables building high availability systems</li>
            <li>Understand the trade-offs involved in building cloud native systems</li>
            <li>Understand the architecture of auto scaling groups</li>
            <li>Know how to create an auto scaling group with Terraform</li>
            <li>Be able to configure hosts created by auto scaling groups</li>
          </ul>
        </section>
        <section>
          <blockquote>Operations at web scale is the ability to consistently create and deploy reliable software to an unreliable platform that scales horizontally.</blockquote>
          &mdash; Jesse Robbins, "Master of Disaster" at Amazon, <a href="http://oreil.ly/1HRKUVE">Operations is a Competitive Advantage</a> (O'Reilly)
        </section>
        <section>
          <h2>Unreliable platform</h2>
          <img style="width: 70%" src="images/oh_noes.png" />
          <aside class="notes">
            This will happen in your physical datacenter too! Amazon mitigates it somewhat with EBS volumes, which have an annual failure rate of 0.1%-0.2%.
          </aside>
        </section>
        <section>
          <h2>Unreliable platform</h2>
          <ul>
            <li class="fragment">If your system is on the internet, you're on an unreliable platform.</li>
            <li class="fragment">EC2 and RDS have a 99.95% SLA for availability...</li>
            <li class="fragment">...but only if you use multiple availability zones.</li>
          </ul>
          <aside class="notes">
            <p>
              SLA = "Service Level Agreement" - they will give you 10% credit for 99.0 - 99.5% uptime; 25% (RDS)-30% (EC2) for under 99%.
            </p>
            <p>
              If we want better than 99.95% availability, we can't depend on AWS's multi-AZ primitives. This is not academic - a whole AWS region became unavailable. The Netflix blog post on the 2011 AWS outage is worth reading: http://techblog.netflix.com/2011/04/lessons-netflix-learned-from-aws-outage.html
            </p>
          </aside>
        </section>
        <section>
          <h2>Scaling horizontally</h2>
          <ul>
            <li class="fragment">Justâ„¢ add more servers.</li>
            <li class="fragment">More automatable than scaling vertically</li>
            <li class="fragment">Avoid shared host-local state.</li>
          </ul>
          <aside class="notes">
            <p>Some performance benefits, and situations where it's a good idea, to have shared state</p>
            <p>This is mostly about the web application tier (and mid-tier if applicable), where most of us work - but some databases are achieving high scalability through horizontal scaling as well (see Cassandra, DynamoDB, Hadoop, any data warehouse...)</p>
          </aside>
        </section>
        <section>
          <h2>Architecting for unreliability</h2>
          <ul>
            <li class="fragment">Hosts should be <em>disposable</em>.</li>
            <li class="fragment">Provisioning new hosts should be fully automated.</li>
            <li class="fragment">Our infrastructure should automatically detect failures and remove hosts.</li>
          </ul>

          <aside class="notes">
            <p>Additional benefit here besides fault tolerance is horizontal scaling.</p>
            <p>
              Clouds are elastic - you can scale on demand, but you have to architect to take advantage of it. Only a stateless architecture truly allows this. For example: keep session state out of the application tier.
            </p>
          </aside>
        </section>
        <section>
          <h2>Introduce failure</h2>
          <p><img style="width:70%" src="images/chubby.jpg"></p>
          <p><i>Site Reliability Engineering: How Google Runs Production Systems</i>, p39.</p>
          </ul>
          <aside class="notes">
            <p>Many of us have heard about Netflix's "Simian Army", including Chaos Monkey, which injects failures in production (!) to make sure systems are prepared to deal w/ failure. It's a fact of life in distributed systems (which we are *all* programming for!), and if we want things to work, we need to test them.</p>
            <p>This story about Chubby is almost more striking because it's about actual people deciding to break their own system so nobody depends on it being too perfect.</p>
          </aside>
        </section>
        <section>
          <h2>What are we building?</h2>
          <img src="images/ha_aws_system.png" alt="system diagram" width="55%" border="0"/>
          <aside class="notes">
            <p>This is the networking infrastructure we'll build and the key components.</p>
            <p>Logging, monitoring, alerting, RDS will have to wait.</p>
          </aside>
        </section>
        <section>
          <h2>Auto Scaling Groups</h2>
          <img src="http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/images/tutorial_as_elb_architecture.png" style="float:right" width="45%"/>
          <ul>
            <li class="fragment">Auto scaling group</li>
            <li class="fragment">Launch configuration</li>
            <li class="fragment">Manual scaling</li>
            <li class="fragment">Scheduled scaling</li>
            <li class="fragment">Dynamic scaling</li>
          </ul>
          <aside class="notes">
            <p>launch config: how you tell ASG what to spin up (including instance type, AMI, user data startup script, etc.)</p>
            <p>Can scale manually (stick to N instances), scheduled (e.g. peak is 9am-11am), dynamic based on metrics (when load is N% CPU)</p>
            <p>ASG has rules about how to scale up and down, how to distribute among AZs</p>
          </aside>
        </section>
        <section>
          <h2>Elastic Load Balancers</h2>
          <img src="http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/images/tutorial_as_elb_architecture.png" style="float:right" width="45%"/>
          <ul>
            <li class="fragment">At least two availability zones</li>
            <li class="fragment">Load balancer</li>
            <li class="fragment">Target group</li>
            <li class="fragment">Health checks</li>
          </ul>
          <aside class="notes">
            <p>ELB lives in multiple AZs, can balance across AZs too.</p>
            <p>target group includes health checks</p>
          </aside>
        </section>
        <section>
          <h2>Elastic Load Balancer types</h2>
          <ul>
            <li class="fragment">Two types: classic and ALBs (application load balancers)</li>
            <li class="fragment">ALB: route based on OSI layer 7 (the "application" layer, e.g. HTTP)</li>
            <li class="fragment">Can route differently depending on things like: headers, content, path</li>
            <li class="fragment">Also supports HTTP/2 and Websockets</li>
            <li class="fragment">ALBs also have request tracing (create a unique header per request)</li>
          </ul>
          <aside class="notes">
            <p>Basically always use ALB unless you need low-level TCP load balancing, which classic ELB can do.</p>
          </aside>
        </section>
        <section>
          <h2>OK let's do this.</h2>
          <p>https://github.com/trptcolin/cloud-native-aws-terraform-workshop</p>
          <p>Where we left off: <code>terraform/terraform.tf</code></p>
          <p>... plus some <code>Name</code> tags for our resources</p>
        </section>
        <section>
          <h2>Second availability zone</h2>
          <pre><code data-noescape>resource "aws_subnet" "public_subnet_2" {
    vpc_id = "${aws_vpc.workshop_vpc.id}"
    availability_zone = "us-east-1b"
    cidr_block = "10.0.1.64/26"
}

resource "aws_route_table_association" "route_subnet_2" {
    subnet_id = "${aws_subnet.public_subnet_2.id}"
    route_table_id = "${aws_route_table.workshop_route_table.id}"
}</code></pre>
          <aside class="notes">
            <p>So in order to balance and scale across multiple AZs, we need another subnet. If you recall from last time, Each subnet is in exactly one AZ.</p>
          </aside>
        </section>
        <section>
          <h2>Security groups: ALB</h2>
          <pre><code data-noescape>resource "aws_security_group" "alb_sg" {
    vpc_id = "${aws_vpc.workshop_vpc.id}"

    ingress {
        from_port = 80
        to_port = 80
        protocol = "tcp"
        cidr_blocks = ["0.0.0.0/0"]
    }

    egress {
        from_port = 80
        to_port = 80
        protocol = "tcp"
        cidr_blocks = ["10.0.1.0/24"]
    }
}</code></pre>
        </section>
        <section>
          <h2>Security groups: Web hosts</h2>
          <pre><code data-noescape>resource "aws_security_group" "web_sg" {
    vpc_id = "${aws_vpc.workshop_vpc.id}"

    ingress {
        from_port = 80
        to_port = 80
        protocol = "tcp"
        security_groups = [ "${aws_security_group.alb_sg.id}" ]
    }

    egress {
        from_port = 0
        to_port = 0
        protocol = "-1"
        cidr_blocks = ["0.0.0.0/0"]
    }
}</code></pre>
        </section>
        <section>
          <h2>Security groups: Web hosts (pt 2)</h2>
          <pre><code data-noescape>    ingress {
        from_port = 22
        to_port = 22
        protocol = "tcp"
        security_groups = [ "${aws_security_group.bastion_sg.id}" ]
    }</code></pre>
        </section>

        <section>
          <h2>A disposal host: incremental steps</h2>
          <pre><code data-noescape>resource "aws_instance" "web_sample" {
    ami = "ami-c481fad3"
    subnet_id = "${aws_subnet.public_subnet_2.id}"
    associate_public_ip_address = true
    key_name = "<mark>[key name]</mark>"
    instance_type = "t2.micro"
    vpc_security_group_ids = ["${aws_security_group.web_sg.id}"]
    tags {
      Name = "SampleWebHost"
    }
}
output "bastion.public_ip" {
  value = "${aws_instance.bastion.public_ip}"
}
output "sample_web_host.public_ip" {
  value = "${aws_instance.web_sample.public_ip}"
}
output "sample_web_host.private_ip" {
  value = "${aws_instance.web_sample.private_ip}"
}</code></pre>
          <aside class="notes">
            <p>This will give us a new EC2 instance, same as the bastion, except:
            <ul>
              <li>Different subnet/AZ (public_subnet_2, us-east-1b)</li>
              <li>Different SG (web_sg instead of bastion_sg)</li>
            </ul>
            </p>
            <p>Also adding outputs here so we can see where to SSH</p>
            <p>I'm more comfortable ensuring small steps work than big ones</p>
          </aside>
        </section>

        <section>
          <h2>Let's give it a shot!</h2>
          <p>Is the security group working? <code>ssh ec2-user@<mark>[sample_web_host.public_ip]</mark> -i ~/.ssh/aws</code></p>
          <p>Does the bastion host allow us to jump in?</p>
          <ul>
            <li><code>ssh ec2-user@<mark>[bastion.public_ip]</mark> -i ~/.ssh/aws</code></li>
            <li>[then, on the bastion host]: <code>ssh <mark>[bastion.private_ip]</mark></code></li>
            <li>"Permission denied (publickey)"</li>
          </ul>
        </section>

        <section>
          <h2>SSH config</h2>
          <p>On your local machine, in <code>~/.ssh/config</code>:</p>
          <pre><code data-noescape>Host <mark>[bastion.public_ip]</mark>
  ForwardAgent yes</code></pre>
          <p>Starting with SSH'ing to the bastion host, try the bastion-to-web jump again!</p>
        </section>

        <section>
          <h2>Now throw the web instance away</h2>
          <p>Remove the <code>web_sample</code> resource and the related <code>output</code>s (you can leave the bastion <code>output</code> in place)</p>
          <p>Then run <code>terraform apply terraform</code> and watch the web instance get destroyed.</p>
        </section>

        <section>
          <h2>Application Load Balancer and Target Group</h2>
          <pre><code data-noescape>resource "aws_alb" "workshop_alb" {
  name = "workshop-alb"
  subnets = ["${aws_subnet.public_subnet_1.id}",
             "${aws_subnet.public_subnet_2.id}"]
  security_groups = ["${aws_security_group.alb_sg.id}"]
}

resource "aws_alb_target_group" "workshop_alb" {
  name = "workshop-alb-target"
  vpc_id = "${aws_vpc.workshop_vpc.id}"
  port = 80
  protocol = "HTTP"
  health_check {
    matcher = "200,301"
    interval = "10"
  }
}
</code></pre>
        </section>
        <section>
          <h2>Wiring up the ALB to the Target Group with a Listener</h2>
<pre><code data-noescape>resource "aws_alb_listener" "workshop_alb_http" {
  load_balancer_arn = "${aws_alb.workshop_alb.arn}"
  port = 80
  protocol = "HTTP"
  default_action {
    target_group_arn = "${aws_alb_target_group.workshop_alb.arn}"
    type = "forward"
  }
}

output "alb.dns" {
    value = "${aws_alb.workshop_alb.dns_name}"
}</code></pre>
  <aside class="notes">
    <p>The output variable <code>dns_name</code> gives us the FQDN of the load balancer.</p>
    <p>ELBs don't promise an IP address, only a DNS name. This lets multi-AZ balancing &amp; fault tolerance work.</p>
    <p>Hit that ALB DNS now, and get a 503. We don't have instances in the target group yet!</p>
  </aside>
        </section>

        <section>
          <h2>Building a web server</h2>
          <pre><code data-noescape>resource "aws_instance" "sample_web_host" {
    ami = "ami-c481fad3"
    subnet_id = "${aws_subnet.public_subnet_2.id}"
    associate_public_ip_address = true
    key_name = "<mark>[key name]</mark>"
    instance_type = "t2.micro"
    vpc_security_group_ids = ["${aws_security_group.web_sg.id}"]
    tags {
      Name = "SampleWebHost"
    }
    user_data = &lt;&lt;EOF
#!/usr/bin/env bash
yum update -y
yum install -y httpd
echo '&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;Hello World!&lt;/body&gt;&lt;/html&gt;' &gt; /var/www/html/index.html
service httpd start
chkconfig httpd on
EOF
}</code></pre>
          <aside class="notes">
            <p>On a real project we might use something like Packer to build a known-good AMI, but we've also got this spot where we can put custom code</p>
          </aside>
        </section>

        <section>
          <h2>Small cycles: put it in the load balancer</h2>
          <pre><code>resource "aws_alb_target_group_attachment" "sample_alb_instance" {
  target_group_arn = "${aws_alb_target_group.workshop_alb.arn}"
  target_id        = "${aws_instance.sample_web_host.id}"
  port             = 80
}</code></pre>
          <p>Now if we run <code>terraform apply terraform</code>, we can hit the load balancer and see the working page!</p>
        </section>

        <section>
          <h2>Auto Scaling: Launch configuration</h2>
          <pre><code data-noescape>resource "aws_launch_configuration" "workshop_launch_conf" {
    image_id = "ami-c481fad3"
    instance_type = "t2.micro"
    key_name = "<mark>[key name]</mark>"
    security_groups = ["${aws_security_group.web_sg.id}"]
    associate_public_ip_address = true
    lifecycle {
        create_before_destroy = true
    }
    user_data = &lt;&lt;EOF
#!/usr/bin/env bash
yum update -y
yum install -y httpd
echo '&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;Hello World!&lt;/body&gt;&lt;/html&gt;' &gt; /var/www/html/index.html
service httpd start
chkconfig httpd on
EOF
}</code></pre>
          <p>Go ahead and remove the sample resources from your terraform file:</p>
          <ul>
            <li>the <code>sample_web_host</code> instance</li>
            <li>the <code>sample_alb_instance</code> target group attachment</li>
          </ul>

          <aside class="notes">
            <p>Very similar to our disposable instance</p>
          </aside>
        </section>
        <section>
          <h2>Auto Scaling Group</h2>
          <pre><code data-noescape>resource "aws_autoscaling_group" "workshop_autoscale" {
    vpc_zone_identifier = ["${aws_subnet.public_subnet_1.id}",
                           "${aws_subnet.public_subnet_2.id}"]
    min_size = 2
    max_size = 2
    health_check_type = "EC2"
    health_check_grace_period = 300
    launch_configuration = "${aws_launch_configuration.workshop_launch_conf.id}"
    target_group_arns = ["${aws_alb_target_group.workshop_alb.arn}"]
    enabled_metrics = ["GroupInServiceInstances"]
}</code></pre>
          <p>Make it so: <code>terraform apply terraform</code></p>
          <aside class="notes">
            <p>Enabling GroupInServiceInstances lets us track the number of in-service instances in the group</p>
          </aside>
        </section>

        <section>
          <h2>Time for some chaos!</h2>
        </section>

        <section>
          <h2>Review</h2>
          <ul>
            <li class="fragment">Internet systems are distributed systems</li>
            <li class="fragment">We need to architect for availability in the face of an unreliable platform</li>
            <li class="fragment">Design for stateless, disposable apps and hosts that can be independently tested and deployed</li>
            <li class="fragment">Be able to provision in a fully automated fashion from version control</li>
            <li class="fragment">We stood up an AWS load balancer and Auto Scaling Group</li>
            <li class="fragment">We deployed all the infrastructure required for a highly available web app</li>
            <li class="fragment">We provisioned everything purely from versioned Terraform config</li>
          </ul>
        </section>

        <section>
          <h1>Thanks!</h1>
          <ul>
            <li>Colin Jones, CTO at 8th Light</li>
            <li><a href="https://twitter.com/trptcolin">@trptcolin</a></li>
          </ul>
          <h2>See you next time?</h2>
          <ul>
            <li>AWS / Terraform Workshop Series #3: Monitoring / Alerting / Log Aggregation</li>
            <li>September 8, same time, <em>different</em> place (25 East Washington)</li>
          </ul>
        </section>

      </div>
    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>
      // More info https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        history: true,

        width: "95%",
        height: "100%",
        margin: 0,
        minScale: 1,
        maxScale: 1,

        // More info https://github.com/hakimel/reveal.js#dependencies
        dependencies: [
          { src: 'plugin/markdown/marked.js' },
          { src: 'plugin/markdown/markdown.js' },
          { src: 'plugin/notes/notes.js', async: true },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
        ]
      });
    </script>
  </body>
</html>
